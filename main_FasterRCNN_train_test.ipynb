{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aGYwt_UjIrqp"
      },
      "source": [
        "# Faster R-CNN Training\n",
        "\n",
        "(1) Train a Detector on A Customized Dataset. \n",
        "\n",
        "(2) The ResNet50 backbone of Faster R-CNN is pre-trained on another dataset in image classification task, implemented in MMPretrain."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tJxJHruNLb7Y"
      },
      "source": [
        "## 1. Check installation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hD0mmMixT0p",
        "outputId": "3fdfddc5-9314-4d11-ed2b-2833795e1cb6"
      },
      "outputs": [],
      "source": [
        "# Step 1. Check Pytorch installation\n",
        "\n",
        "import torch, torchvision\n",
        "print(\"torch version: \", torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMDetection installation\n",
        "import mmdet\n",
        "print(\"mmdet version: \", mmdet.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "print(get_compiling_cuda_version())\n",
        "print(get_compiler_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2. We need to download config and checkpoint files.\n",
        "\n",
        "# The downloading will take several seconds or more, depending on your network environment. \n",
        "# When it is done, you will find two files rtmdet_tiny_8xb32-300e_coco.py，\n",
        "#  and rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth in your current folder.\n",
        "\n",
        "!mim download mmdet --config rtmdet_tiny_8xb32-300e_coco --dest ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 (Case a): \n",
        "\n",
        "# If you install MMDetection from source, just run the following command.\n",
        "# You will see a new image demo.jpg on your ./outputs/vis folder, \n",
        "# where bounding boxes are plotted on cars, benches, etc.\n",
        "\n",
        "!python demo/image_demo.py demo/demo.jpg rtmdet_tiny_8xb32-300e_coco.py --weights rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --device cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 (Case b): \n",
        "\n",
        "# If you install MMDetection with MIM, open your python interpreter and copy&paste the following codes.\n",
        "# You will see a list of DetDataSample, and the predictions are in the pred_instance, indicating the detected bounding boxes, labels, and scores.\n",
        "\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "\n",
        "config_file = 'rtmdet_tiny_8xb32-300e_coco.py'\n",
        "checkpoint_file = 'rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
        "model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
        "inference_detector(model, 'demo/demo.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare the dataset in COCO format\n",
        "\n",
        "MMDetection supports multiple public datasets including COCO, Pascal VOC, CityScapes, and more.\n",
        "\n",
        "Here we only use the COCO format dataset, as follows:\n",
        "\n",
        "```\n",
        "data_root\n",
        "├──annotations (folder)\n",
        "├  ├── train.json\n",
        "├  ├── val.json\n",
        "├  └── \n",
        "├──train (folder)\n",
        "├  ├── 1.jpg\n",
        "├  ├── 2.jpg\n",
        "├  └── n.jpg\n",
        "├──val (folder)\n",
        "├  ├── 1.jpg\n",
        "├  └── 2.jpg\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PwqJOpBe-bMj"
      },
      "source": [
        "## 3. Modify the model config\n",
        "\n",
        "We generate a file **configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco_deep_plastic_mmpretrain.py**\n",
        "\n",
        "where we may update the following values/parameter if possible:\n",
        "\n",
        "1. Class name: \"metainfo\".\n",
        "\n",
        "2. Dataset path: (1) \"data_root\"; (2) \"ann_file\"; (3) \"data_prefix\".\n",
        "\n",
        "3. Output path： \"work_dir\".\n",
        "\n",
        "4. Model architecture: \n",
        "\n",
        "(1) pretrained model weight: \"backbone.init_cfg.checkpoint\"; \n",
        "\n",
        "(2) num-classes: \"roi_head.bbox_head.num_classes\";\n",
        "\n",
        "(3) Epoches: \"train_cfg.max_epochs\"; \n",
        "\n",
        "(4) batch_size: \"train_dataloader.batch_size\", and \"val_dataloader.batch_size\".\n",
        "\n",
        "(5) Save the best model with a specific evaluation metric by changing \"default_hooks.checkpoint.save_best\"\n",
        "\n",
        "The metrics include:\n",
        "\n",
        "(a) For object detection: 'bbox_mAP', 'bbox_mAP_50', 'bbox_mAP_75', 'bbox_mAP_s', 'bbox_mAP_m', 'bbox_mAP_l', 'bbox_mAP_copypaste'\n",
        "\n",
        "(b) For instance segmentation: 'segm_mAP', 'segm_mAP_50', 'segm_mAP_75', 'segm_mAP_s', 'segm_mAP_m', 'segm_mAP_l', 'segm_mAP_copypaste'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train a new detector\n",
        "\n",
        "Finally, lets initialize the dataset and detector, then train a new detector!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 ResNet50 pretrained on GV dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ResNet50 pretrained on GV dataset\n",
        "!python tools/train.py \\\n",
        "    configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco_deep_plastic_mmpretrain.py \\\n",
        "    --work-dir /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp4/Plan_C/Jak_100per/F4_PreTrain_GV/ \\\n",
        "    --cfg-options data_root='/scratch/tjian/Data/Paper2_Exp5/Plan_C/Jak_100per/' model.backbone.frozen_stages=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 ResNet50 pretrained on ImageNet dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ResNet50 Pretrained on ImageNet dataset\n",
        "!python tools/train.py \\\n",
        "    configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco_deep_plastic_Pretrain_ImgaeNet.py \\\n",
        "    --work-dir /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp4/Plan_C/Jak_100per/F4_PreTrain_ImageNet/ \\\n",
        "    --cfg-options data_root='/scratch/tjian/Data/Paper2_Exp5/Plan_C/Jak_100per/' model.backbone.frozen_stages=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single-gpu testing\n",
        "\n",
        "# \"--show-dir\" saves the predicted images in test dataset\n",
        "\n",
        "!python tools/test.py \\\n",
        "    configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco_deep_plastic_mmpretrain.py \\\n",
        "    /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp4/Plan_C/Jak_100per/F4_PreTrain_GV/best_coco_bbox_mAP_50_epoch_100.pth \\\n",
        "    --work-dir /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp4/Plan_C/Jak_100per/F4_PreTrain_GV/\n",
        "    # --show-dir /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp5/Oos_20per/FRCNN_Frozen_4_PreTrain_GV/pred_images_in_test_dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/test.py \\\n",
        "    configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco_deep_plastic_Pretrain_ImgaeNet.py \\\n",
        "    /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp4/Plan_C/Jak_100per/F4_PreTrain_ImageNet/best_coco_bbox_mAP_50_epoch_78.pth \\\n",
        "    --work-dir /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp4/Plan_C/Jak_100per/F4_PreTrain_ImageNet/\n",
        "    # --show-dir /scratch/tjian/PythonProject/DP_MMDetection/checkpoints/train_weights/Paper2_exp5/Oos_20per/FRCNN_Frozen_4_PreTrain_ImageNet/pred_images_in_test_dataset/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load tensorboard\n",
        "\n",
        "Understand the log\n",
        "\n",
        "From the log, we can have a basic understanding on the training process and know how well the detector is trained.\n",
        "\n",
        "First, since the dataset we are using is small, we loaded a Mask R-CNN model and finetune it for detection. Because the original Mask R-CNN is trained on COCO dataset that contains 80 classes but KITTI Tiny dataset only have 3 classes. Therefore, the last FC layers of the pre-trained Mask R-CNN for classification and regression have different weight shape and are not used. The pre-trained weights of mask prediction layer `mask_head.conv_logits` also does not matches the current model and is not used due to similar reason.\n",
        "\n",
        "Third, after training, the detector is evaluated by the default COCO-style evaluation. The results show that the detector achieves 79.6 bbox AP and 81.5 mask AP on the val dataset, not bad!\n",
        "\n",
        " We can also check the tensorboard to see the curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load tensorboard in colab\n",
        "%load_ext tensorboard\n",
        "\n",
        "# if the tensorboard page on VS Code is not so clear, \n",
        "# you can type this (localhost:6006) on web browser after executing this code \n",
        "\n",
        "# see curves in tensorboard\n",
        "%tensorboard --logdir ./tutorial_exps"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tutorial_03_image_segmentation_final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "openmmlab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "edd6c80a9e89c79aec98202bf9aae967f818a805fb6d102d63b47886e3a3e88e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
